{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tempfile\n",
    "import ollama\n",
    "import shutil\n",
    "import subprocess\n",
    "import pytesseract\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, poppler_path=None, dpi=200):\n",
    "        self.poppler_path = poppler_path\n",
    "        self.dpi = dpi\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        images = convert_from_path(pdf_path, dpi=self.dpi, poppler_path=self.poppler_path)\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            texts = list(executor.map(pytesseract.image_to_string, images))\n",
    "        return texts\n",
    "\n",
    "    def extract_text_from_image(self, image_path):\n",
    "        return pytesseract.image_to_string(Image.open(image_path))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        return [p.strip() for p in text.split('\\n\\n') if len(p.strip()) > 5]\n",
    "\n",
    "    def create_documents_pdf(self, text_pages, pdf_path):\n",
    "        documents = []\n",
    "        for page_num, page_text in enumerate(text_pages):\n",
    "            for para_num, para in enumerate(self.clean_text(page_text)):\n",
    "                doc_id = f\"pdf_{os.path.basename(pdf_path)}_{page_num}_{para_num}\"\n",
    "                documents.append({\n",
    "                    \"id\": doc_id,\n",
    "                    \"text\": para,\n",
    "                    \"metadata\": {\"source\": \"pdf\"}\n",
    "                })\n",
    "        return documents\n",
    "\n",
    "    def create_documents_image(self, text, image_path):\n",
    "        cleaned_paragraph = ' '.join([p.strip() for p in text.split('\\n\\n') if p.strip()])\n",
    "        doc_id = f\"image_{os.path.basename(image_path)}\"\n",
    "        return [{\n",
    "            \"id\": doc_id,\n",
    "            \"text\": cleaned_paragraph,\n",
    "            \"metadata\": {\"source\": \"figure\"}\n",
    "        }]\n",
    "\n",
    "\n",
    "class KeywordGenerator:\n",
    "    def __init__(self, model_name=\"mistral:7b-instruct-v0.2-q5_K_M\"):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def generate_keywords(self, text):\n",
    "        prompt = \"Summarize the following paragraph into 3 keywords separated by commas: \" + text\n",
    "        response = ollama.generate(model=self.model_name, prompt=prompt, options={\"temperature\": 0.1})[\"response\"]\n",
    "        return [kw.strip() for kw in response.split(\",\") if len(kw.strip()) > 2]\n",
    "\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, db_path=\"rag_db\"):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "        self.index = None\n",
    "        self.embeddings = None\n",
    "        self.db_path = db_path\n",
    "        self.documents = []\n",
    "        self._load_database()\n",
    "\n",
    "    def _load_database(self):\n",
    "        vectorizer_path = os.path.join(self.db_path, \"vectorizer.pkl\")\n",
    "        index_path = os.path.join(self.db_path, \"index.faiss\")\n",
    "        docs_path = os.path.join(self.db_path, \"docs.pkl\")\n",
    "\n",
    "        # Load documents\n",
    "        if os.path.exists(docs_path):\n",
    "            with open(docs_path, \"rb\") as f:\n",
    "                self.documents = pickle.load(f)\n",
    "\n",
    "        # Load FAISS index\n",
    "        if os.path.exists(index_path):\n",
    "            self.index = faiss.read_index(index_path)\n",
    "\n",
    "        # Load or fit vectorizer\n",
    "        if os.path.exists(vectorizer_path):\n",
    "            with open(vectorizer_path, \"rb\") as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "        else:\n",
    "            # Retroactively fit on existing documents\n",
    "            if self.documents:\n",
    "                texts = [doc[\"text\"] for doc in self.documents]\n",
    "                self.vectorizer.fit(texts)\n",
    "            os.makedirs(self.db_path, exist_ok=True)\n",
    "            with open(vectorizer_path, \"wb\") as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "\n",
    "    def _save_database(self):\n",
    "        docs_path = os.path.join(self.db_path, \"docs.pkl\")\n",
    "        index_path = os.path.join(self.db_path, \"index.faiss\")\n",
    "        vectorizer_path = os.path.join(self.db_path, \"vectorizer.pkl\")\n",
    "\n",
    "        # Save documents\n",
    "        with open(docs_path, \"wb\") as f:\n",
    "            pickle.dump(self.documents, f)\n",
    "\n",
    "        # Save FAISS index\n",
    "        if self.index is not None:\n",
    "            faiss.write_index(self.index, index_path)\n",
    "\n",
    "        # Save vectorizer\n",
    "        with open(vectorizer_path, \"wb\") as f:\n",
    "            pickle.dump(self.vectorizer, f)\n",
    "\n",
    "    def create_embeddings(self, documents):\n",
    "        texts = [doc[\"text\"] for doc in documents]\n",
    "        embeddings = self.vectorizer.fit_transform(texts).toarray()\n",
    "        self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        self.index.add(embeddings.astype(\"float32\"))\n",
    "        self.documents.extend(documents)\n",
    "        self._save_database()\n",
    "\n",
    "    def add_documents(self, new_documents):\n",
    "        new_texts = [doc[\"text\"] for doc in new_documents]\n",
    "        new_embeddings = self.vectorizer.transform(new_texts).toarray()\n",
    "        self.index.add(new_embeddings.astype(\"float32\"))\n",
    "        self.documents.extend(new_documents)\n",
    "        self._save_database()\n",
    "\n",
    "    def search(self, query, top_k=1):\n",
    "        query_embedding = self.vectorizer.transform([query]).toarray().astype(\"float32\")\n",
    "        _, indices = self.index.search(query_embedding, top_k)\n",
    "        return indices[0]\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, poppler_path=None):\n",
    "        self.processor = DocumentProcessor(poppler_path=poppler_path)\n",
    "        self.keyword_gen = KeywordGenerator()\n",
    "        self.embedding_manager = EmbeddingManager()\n",
    "        self.documents = self.embedding_manager.documents\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        text_pages = self.processor.extract_text_from_pdf(pdf_path)\n",
    "        pdf_docs = self.processor.create_documents_pdf(text_pages, pdf_path)\n",
    "        self._add_documents(pdf_docs)\n",
    "        self._process_embedded_images(pdf_path)\n",
    "\n",
    "    def _process_embedded_images(self, pdf_path):\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        try:\n",
    "            output_prefix = os.path.join(temp_dir, \"image\")\n",
    "            pdfimages_path = os.path.join(self.processor.poppler_path, \"pdfimages\")\n",
    "            args = [pdfimages_path, \"-all\", pdf_path, output_prefix]\n",
    "            subprocess.run(args, capture_output=True, text=True)\n",
    "            image_files = glob.glob(os.path.join(temp_dir, \"image-*\"))\n",
    "            for img_path in image_files:\n",
    "                try:\n",
    "                    self.process_image(img_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing embedded image {img_path}: {str(e)}\")\n",
    "        finally:\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        text = self.processor.extract_text_from_image(image_path)\n",
    "        image_docs = self.processor.create_documents_image(text, image_path)\n",
    "        self._add_documents(image_docs)\n",
    "\n",
    "    def _add_documents(self, new_docs):\n",
    "        for doc in tqdm(new_docs, desc=\"Processing documents\"):\n",
    "            doc[\"metadata\"][\"keywords\"] = self.keyword_gen.generate_keywords(doc[\"text\"])\n",
    "        if not self.embedding_manager.index:\n",
    "            self.embedding_manager.create_embeddings(new_docs)\n",
    "        else:\n",
    "            self.embedding_manager.add_documents(new_docs)\n",
    "\n",
    "    def process_directory(self, directory_path):\n",
    "        pdf_files = glob.glob(os.path.join(directory_path, \"*.pdf\"))\n",
    "        image_files = glob.glob(os.path.join(directory_path, \"*.png\")) + glob.glob(os.path.join(directory_path, \"*.jpg\"))\n",
    "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            self.process_pdf(pdf_path)\n",
    "        for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "            self.process_image(img_path)\n",
    "\n",
    "    def search(self, query, top_k=1):\n",
    "        indices = self.embedding_manager.search(query, top_k)\n",
    "        return [{\n",
    "            \"document\": self.documents[idx][\"text\"],\n",
    "            \"metadata\": self.documents[idx][\"metadata\"]\n",
    "        } for idx in indices if 0 <= idx < len(self.documents)]\n",
    "\n",
    "    def generate_answer(self, query, use_context=True):\n",
    "        context_docs = self.search(query)\n",
    "        context = \" \".join([doc[\"document\"] for doc in context_docs])\n",
    "        prompt = f\"Answer using {'ONLY the following context' if use_context else 'your knowledge and this additional context'}:\\n{context}\\n\\nQuestion: {query}\"\n",
    "        response = ollama.chat(\n",
    "            model=\"mistral:7b-instruct-v0.2-q5_K_M\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=False\n",
    "        )\n",
    "        return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust these paths accordingly\n",
    "    poppler_path = r\"C:\\\\Users\\\\Dr.Wael Abouelwafa\\\\Downloads\\\\poppler-24.08.0\\\\Library\\\\bin\"\n",
    "    rag = RAGPipeline(poppler_path=poppler_path)\n",
    "    \n",
    "    # Process directory\n",
    "    rag.process_directory(r\"C:\\\\Users\\\\Dr.Wael Abouelwafa\\\\Desktop\\\\graduation project 2025\\\\all data in one file\")\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
